{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca855dc7",
   "metadata": {},
   "source": [
    "# 四轮驱动转向架正交参数化动力学仿真结果分析\n",
    "核心功能覆盖 7 个步骤：\n",
    "1) 数据整形 data reshaping（转置 Excel → 整洁表 tidy DataFrame）、中英列名别名解析与模糊匹配  \n",
    "2) 派生变量重算：D_calc = Lx1 - Lx2、DiffWear_calc = RW Wear - IRW Wear  \n",
    "3) 质量校验与范围检查 data validation  \n",
    "4) 皮尔逊相关 Pearson correlation 与 部分相关 partial correlation（控制 controls，默认 Kpx）  \n",
    "5) 一阶回归与交互（statsmodels OLS + HC3 标准误），含 poly(Lx3,2) 与交互（Lx3×Kpx/Lx1/Lx2/Chx）  \n",
    "6) 必要散点与拟合线\n",
    "7) VIF（方差膨胀因子 Variance Inflation Factor）与随机森林 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b2b3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 环境与显示设置\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "import logging\n",
    "import platform\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 可选依赖检测（不自动安装，保持与脚本一致的“优雅降级”）\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    import statsmodels.formula.api as smf\n",
    "    HAVE_SM = True\n",
    "except Exception:\n",
    "    HAVE_SM = False\n",
    "\n",
    "try:\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    HAVE_SK = True\n",
    "except Exception:\n",
    "    HAVE_SK = False\n",
    "\n",
    "try:\n",
    "    from scipy import stats as spstats\n",
    "    HAVE_SCIPY = True\n",
    "except Exception:\n",
    "    HAVE_SCIPY = False\n",
    "\n",
    "# patsy Q：公式中安全引用含空格/符号的列名\n",
    "try:\n",
    "    from patsy.builtins import Q\n",
    "    HAVE_Q = True\n",
    "except Exception:\n",
    "    HAVE_Q = False\n",
    "\n",
    "# Matplotlib：启用 inline 或 widget（交互）\n",
    "from IPython import get_ipython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# —— 字体与负号：中文显示（Chinese font fallback） ——\n",
    "def _pick_first_installed(candidates):\n",
    "    installed = {f.name for f in matplotlib.font_manager.fontManager.ttflist}\n",
    "    for name in candidates:\n",
    "        if name in installed:\n",
    "            return name\n",
    "    return None\n",
    "\n",
    "def setup_matplotlib_chinese():\n",
    "    sys_name = platform.system()\n",
    "    if sys_name == \"Windows\":\n",
    "        candidates = [\"Microsoft YaHei\", \"SimHei\"]\n",
    "    elif sys_name == \"Darwin\":\n",
    "        candidates = [\"PingFang SC\", \"Heiti SC\", \"Songti SC\"]\n",
    "    else:\n",
    "        candidates = [\"Noto Sans CJK SC\", \"WenQuanYi Zen Hei\", \"AR PL UMing CN\"]\n",
    "    chosen = _pick_first_installed(candidates) or \"DejaVu Sans\"\n",
    "    matplotlib.rcParams[\"font.sans-serif\"] = [chosen, \"DejaVu Sans\"]\n",
    "    matplotlib.rcParams[\"axes.unicode_minus\"] = False\n",
    "    return sys_name, chosen\n",
    "\n",
    "# 默认 inline；如需 3D 交互可改为 \"widget\"\n",
    "BACKEND_MODE = \"inline\"  # 或 \"widget\"\n",
    "ip = get_ipython()\n",
    "if ip:\n",
    "    ip.run_line_magic(\"matplotlib\", BACKEND_MODE)\n",
    "\n",
    "sys_name, font_used = setup_matplotlib_chinese()\n",
    "print(f\"[Info] OS={sys_name}, Matplotlib backend={matplotlib.get_backend()}, 中文字体={font_used}\")\n",
    "print(f\"[Info] HAVE_SM={HAVE_SM}, HAVE_SK={HAVE_SK}, HAVE_SCIPY={HAVE_SCIPY}, HAVE_Q={HAVE_Q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e099472",
   "metadata": {},
   "source": [
    "# 基本参数（路径、表名、ROI、控制变量等）\n",
    "\n",
    "- 请根据实际文件修改 `INPUT_XLSX`、`SHEET_NAME`、`OUTDIR`。  \n",
    "- ROI（感兴趣区间 region of interest）字符串示例：`\"Lx1:[0.60,0.64];Lx2:[0,0.04];Lx3:[-0.6,-0.1]\"`  \n",
    "- 控制变量 controls 默认 [\"Kpx\"]（可多选）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3708cb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置路径与参数\n",
    "INPUT_XLSX = Path(r\"F:\\ResearchMainStream\\0.ResearchBySection\\C.动力学模型\\C23参数优化\\参数优化实现\\ParallelSweepSimpack\\ParallelSweep_OrthogonalDoE.xlsx\")\n",
    "SHEET_NAME = \"Sheet1\"   # 或 None 使用首个工作表\n",
    "OUTDIR = Path.cwd() / \"Results_OrthogonalDoE\"\n",
    "KEEP_POLICY = \"any\"     # 'any' 或 'opt_derived'\n",
    "CONTROLS = [\"Kpx\"]      # 部分相关/回归中的控制变量\n",
    "ROI_STRING = \"Lx1:[0.60,0.64];Lx2:[0,0.04];Lx3:[-0.6,-0.1]\"  # 可为空字符串\n",
    "\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Check] INPUT:\", INPUT_XLSX, \"存在:\", INPUT_XLSX.exists())\n",
    "print(\"[Check] OUTDIR:\", OUTDIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffab7814",
   "metadata": {},
   "source": [
    "# 函数库 · 读取与整形（Utilities: Reshaping）\n",
    "工具函数（日志、保存、别名解析、读取转置 Excel、转整洁表）\n",
    "- 别名解析 alias resolution：对列名进行标准化（大小写/空白/符号清理）并支持前缀/子串匹配，以应对“列名截断”问题。\n",
    "- 转置 Excel → 整洁表：A–D 列为元信息（`row_type/eng/zh/flag`），E.. 为数值；按 `eng` 去重，优先保留 `row_type ∈ {DerivedQuantity, Derived, opt}`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f677855e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取/整形工具函数\n",
    "def setup_logging(outdir: Path, level=logging.INFO) -> None:\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    logging.basicConfig(\n",
    "        level=level,\n",
    "        format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(outdir / \"run.log\", encoding=\"utf-8\"),\n",
    "            logging.StreamHandler(sys.stdout),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "def save_csv(df: pd.DataFrame, path: Path) -> None:\n",
    "    df.to_csv(path, index=False)\n",
    "\n",
    "def save_json(obj: dict, path: Path) -> None:\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def normalize_token(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = str(s).strip().lower()\n",
    "    s = re.sub(r\"[\\s_()\\[\\]\\-–—]+\", \"\", s)\n",
    "    return s\n",
    "\n",
    "def first_present(candidates: List[str], available: List[str]) -> Optional[str]:\n",
    "    avail_norm = {normalize_token(a): a for a in available}\n",
    "    cand_norm = [normalize_token(c) for c in candidates]\n",
    "    for cn in cand_norm:\n",
    "        if cn in avail_norm:\n",
    "            return avail_norm[cn]\n",
    "    for cn in cand_norm:\n",
    "        for norm_a, a in avail_norm.items():\n",
    "            if norm_a.startswith(cn) or cn.startswith(norm_a) or (cn in norm_a):\n",
    "                return a\n",
    "    return None\n",
    "\n",
    "CANONICAL_SYNONYMS: Dict[str, List[str]] = {\n",
    "    # Geometry / design knobs\n",
    "    \"Lx1\": [\"$_Lx1\", \"Lx1\", \"Lx_1\", \"Lx 1\", \"Lx1 (mm)\", \"Lx1(mm)\"],\n",
    "    \"Lx2\": [\"$_Lx2\", \"Lx2\", \"Lx_2\", \"Lx 2\", \"Lx2 (mm)\", \"Lx2(mm)\"],\n",
    "    \"Lx3\": [\"$_Lx3\", \"Lx3\", \"Lx_3\", \"Lx 3\", \"Local_Lx3\", \"Local Lx3\", \"Local-Lx3\"],\n",
    "    \"Kpx\": [\"$_Kpx\", \"Kpx\", \"K_px\", \"K px\"],\n",
    "    \"Chx\": [\"$_Chx\", \"Chx\", \"C_hx\", \"C hx\"],\n",
    "    \"Cld\": [\"$_Cld\", \"Cld\", \"C_ld\", \"C ld\"],\n",
    "    \"Kld\": [\"$_Kld\", \"Kld\", \"K_ld\", \"K ld\"],\n",
    "    \"Ksx\": [\"$_Ksx\", \"Ksx\", \"K_sx\", \"K sx\"],\n",
    "    # Wear\n",
    "    \"RW Wear - Curve\": [\"RW Wear - Curve\",\"RW Wear–Curve\",\"RW Wear – Curve\",\"RW_Wear_Curve\",\"RW Wear(Curve)\",\"RW Wear Curve\",\"RW Wear-Curve\"],\n",
    "    \"IRW Wear - Curve\": [\"IRW Wear - Curve\",\"IRW Wear–Curve\",\"IRW Wear – Curve\",\"IRW_Wear_Curve\",\"IRW Wear(Curve)\",\"IRW Wear Curve\",\"IRW Wear-Curve\"],\n",
    "    # Derived\n",
    "    \"DiffWear\": [\"DiffWear\",\"Diff Wear\",\"RW-IRW Wear\",\"RW-IRW\",\"RW minus IRW\",\"DiffWear=RW-IRW Wear\",\"ΔWear\",\"DeltaWear\"],\n",
    "    \"D\": [\"D\",\"Lx1-Lx2\",\"Lx1_minus_Lx2\",\"D=Lx1-Lx2\"],\n",
    "    # Performance / comfort\n",
    "    \"RW Critical Vel - STR\": [\"RW Critical Vel - STR\",\"RW Critical Velocity - STR\",\"RW_crit_vel_STR\",\"RW Critical Vel STR\",\"RW Critical Vel-STR\"],\n",
    "    \"RW SperlingY - STR\": [\"RW SperlingY - STR\",\"RW Sperling Y - STR\",\"RW_SperlingY_STR\",\"SperlingY - STR (RW)\",\"SperlingY_STR_RW\"],\n",
    "    \"RW SperlingZ - STR\": [\"RW SperlingZ - STR\",\"RW Sperling Z - STR\",\"RW_SperlingZ_STR\",\"SperlingZ - STR (RW)\",\"SperlingZ_STR_RW\"],\n",
    "    \"RBC\": [\"RBC\",\"R.B.C\",\"RBC_value\"],\n",
    "}\n",
    "\n",
    "def resolve_aliases(columns: List[str], custom_aliases: Optional[Dict[str, List[str]]] = None) -> Dict[str, Optional[str]]:\n",
    "    aliases = dict(CANONICAL_SYNONYMS)\n",
    "    if custom_aliases:\n",
    "        for k, v in custom_aliases.items():\n",
    "            aliases.setdefault(k, [])\n",
    "            aliases[k] = list(dict.fromkeys(aliases[k] + v))\n",
    "    mapping = {}\n",
    "    for canon, cands in aliases.items():\n",
    "        found = first_present(cands + [canon], columns)\n",
    "        mapping[canon] = found\n",
    "    return mapping\n",
    "\n",
    "def numeric_coerce(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "def load_transposed_excel(path: Path, sheet: Optional[str]) -> pd.DataFrame:\n",
    "    df = pd.read_excel(path, sheet_name=sheet, header=None, engine=\"openpyxl\")\n",
    "    df = df.replace({r\"^\\s*$\": np.nan}, regex=True)\n",
    "    return df\n",
    "\n",
    "@dataclass\n",
    "class TidyResult:\n",
    "    tidy: pd.DataFrame\n",
    "    row_meta: pd.DataFrame\n",
    "    used_rows: pd.DataFrame\n",
    "    info: dict\n",
    "\n",
    "def transposed_to_tidy(\n",
    "    df_raw: pd.DataFrame,\n",
    "    keep_policy: str = \"any\",\n",
    "    prefer_row_type: Tuple[str, ...] = (\"DerivedQuantity\", \"Derived\", \"opt\")\n",
    ") -> TidyResult:\n",
    "    meta = df_raw.iloc[:, :4].copy()\n",
    "    meta.columns = [\"row_type\", \"eng\", \"zh\", \"flag\"]\n",
    "    runs = df_raw.iloc[:, 4:].copy()\n",
    "    runs_num = numeric_coerce(runs)\n",
    "\n",
    "    m_eng = meta[\"eng\"].notna()\n",
    "    m_numeric = runs_num.apply(lambda r: r.notna().any(), axis=1)\n",
    "    if keep_policy == \"opt_derived\":\n",
    "        m_type = meta[\"row_type\"].astype(str).str.lower().str.startswith((\"opt\", \"derived\"))\n",
    "        keep = m_eng & m_numeric & m_type\n",
    "    else:\n",
    "        keep = m_eng & m_numeric\n",
    "\n",
    "    df_keep = pd.concat([meta.loc[keep, [\"row_type\",\"eng\",\"zh\",\"flag\"]],\n",
    "                         runs_num.loc[keep, :]], axis=1)\n",
    "\n",
    "    def pref_order(s):\n",
    "        s = str(s) if pd.notna(s) else \"\"\n",
    "        s_low = s.lower()\n",
    "        for i, p in enumerate(prefer_row_type):\n",
    "            if s_low.startswith(p.lower()):\n",
    "                return i\n",
    "        return len(prefer_row_type)\n",
    "\n",
    "    df_keep[\"pref_rank\"] = df_keep[\"row_type\"].map(pref_order)\n",
    "    df_keep = df_keep.sort_values(by=[\"eng\",\"pref_rank\"]).drop_duplicates(subset=[\"eng\"], keep=\"first\")\n",
    "    df_keep = df_keep.drop(columns=[\"pref_rank\"])\n",
    "\n",
    "    run_values = df_keep.iloc[:, 4:].copy()\n",
    "    run_values.index = df_keep[\"eng\"].astype(str).str.strip()\n",
    "    wide = run_values.T.copy()\n",
    "    wide.index.name = \"sample_id\"\n",
    "    wide.reset_index(inplace=True)\n",
    "    wide[\"sample_id\"] = wide.index.astype(int)\n",
    "    tidy = wide.copy()\n",
    "\n",
    "    info = {\"n_runs\": int(tidy.shape[0]), \"n_vars\": int(tidy.shape[1]-1),\n",
    "            \"keep_policy\": keep_policy, \"prefer_row_type\": list(prefer_row_type)}\n",
    "\n",
    "    return TidyResult(\n",
    "        tidy=tidy,\n",
    "        row_meta=df_keep[[\"row_type\",\"eng\",\"zh\",\"flag\"]].reset_index(drop=True),\n",
    "        used_rows=df_keep,\n",
    "        info=info\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8ac60a",
   "metadata": {},
   "source": [
    "# 函数库 · 分析与可视化（Utilities: Analytics）\n",
    "分析函数（相关/部分相关、VIF、散点拟合、OLS、RF、ROI）\n",
    "\n",
    "- partial correlation：残差化 residualization 后计算相关系数；可选 `SciPy` 给出 p 值。  \n",
    "- OLS（HC3）：`y ~ poly(Lx3,2) + controls + interactions(Lx3:Z)`；保存系数表与摘要。  \n",
    "- 随机森林：若 `scikit-learn` 可用，计算置换重要性 permutation importance。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15657602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析工具函数\n",
    "def pearson_corr(df: pd.DataFrame, cols: Sequence[str]) -> pd.DataFrame:\n",
    "    sub = df[list(cols)].copy()\n",
    "    return sub.corr(method=\"pearson\")\n",
    "\n",
    "def partial_corr(df: pd.DataFrame, x: str, y: str, controls: Sequence[str]) -> Tuple[float, Optional[float], int]:\n",
    "    cols = [x, y] + list(controls)\n",
    "    sub = df[cols].dropna()\n",
    "    n = sub.shape[0]\n",
    "    if n < (len(controls) + 3):\n",
    "        return np.nan, None, n\n",
    "\n",
    "    Xc = sub[list(controls)].values\n",
    "    Xc = np.column_stack([np.ones(len(Xc)), Xc]) if Xc.size else np.ones((len(sub), 1))\n",
    "\n",
    "    def resid(v: np.ndarray) -> np.ndarray:\n",
    "        beta, *_ = np.linalg.lstsq(Xc, v, rcond=None)\n",
    "        return v - Xc @ beta\n",
    "\n",
    "    rx = resid(sub[x].values.astype(float))\n",
    "    ry = resid(sub[y].values.astype(float))\n",
    "    r = np.corrcoef(rx, ry)[0, 1]\n",
    "\n",
    "    pval = None\n",
    "    if HAVE_SCIPY and np.isfinite(r):\n",
    "        k = len(controls)\n",
    "        dfree = n - k - 2\n",
    "        if dfree > 0 and abs(r) < 1:\n",
    "            tstat = r * math.sqrt(dfree / (1 - r**2))\n",
    "            pval = 2 * spstats.t.sf(abs(tstat), df=dfree)\n",
    "    return float(r), (float(pval) if pval is not None else None), int(n)\n",
    "\n",
    "def vif_table(df: pd.DataFrame, cols: Sequence[str]) -> Optional[pd.DataFrame]:\n",
    "    if not HAVE_SM:\n",
    "        return None\n",
    "    X = df[list(cols)].dropna()\n",
    "    if X.shape[1] < 2 or X.shape[0] <= X.shape[1]:\n",
    "        return None\n",
    "    X = sm.add_constant(X)\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "    vifs = []\n",
    "    for i, name in enumerate(X.columns):\n",
    "        if name == \"const\":\n",
    "            continue\n",
    "        vifs.append({\"feature\": name, \"VIF\": variance_inflation_factor(X.values, i)})\n",
    "    return pd.DataFrame(vifs)\n",
    "\n",
    "def scatter_with_fit(df: pd.DataFrame, x: str, y: str, outpath: Path, title: Optional[str] = None, show: bool = True):\n",
    "    sub = df[[x, y]].dropna()\n",
    "    if sub.shape[0] < 2:\n",
    "        return\n",
    "    xvals = sub[x].values\n",
    "    yvals = sub[y].values\n",
    "    coeff = np.polyfit(xvals, yvals, 1)\n",
    "    xgrid = np.linspace(np.nanmin(xvals), np.nanmax(xvals), 200)\n",
    "    yfit = coeff[0] * xgrid + coeff[1]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(xvals, yvals, s=16)\n",
    "    plt.plot(xgrid, yfit, linewidth=2)\n",
    "    plt.xlabel(x); plt.ylabel(y)\n",
    "    plt.title(title or f\"{x} vs {y}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath, dpi=200)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def ols_lx3_model(df: pd.DataFrame, y: str, controls: Sequence[str], interactions: Sequence[Tuple[str, str]], out_prefix: Path) -> Optional[dict]:\n",
    "    if not HAVE_SM:\n",
    "        logging.warning(\"statsmodels not installed; OLS step skipped.\")\n",
    "        return None\n",
    "\n",
    "    pieces = [\"1\", \"np.power(Lx3,1)\", \"np.power(Lx3,2)\"]\n",
    "    for c in controls:\n",
    "        pieces.append(c)\n",
    "    for a, b in interactions:\n",
    "        pieces.append(f\"{a}:{b}\")\n",
    "        if a not in controls and a != \"Lx3\":\n",
    "            pieces.append(a)\n",
    "        if b not in controls and b != \"Lx3\":\n",
    "            pieces.append(b)\n",
    "\n",
    "    uniq = []\n",
    "    for p in pieces:\n",
    "        if p not in uniq:\n",
    "            uniq.append(p)\n",
    "\n",
    "    # 若 patsy.Q 可用，用 Q(\"y\") 防止目标名含空格\n",
    "    lhs = f'Q(\"{y}\")' if HAVE_Q else y\n",
    "    formula = f'{lhs} ~ ' + \" + \".join(uniq)\n",
    "\n",
    "    need_cols = [\"Lx3\", y] + list(controls) + [b for a, b in interactions if b not in controls and b != \"Lx3\"]\n",
    "    sub = df[need_cols].dropna()\n",
    "    if sub.shape[0] < 12:\n",
    "        logging.warning(\"Too few rows for OLS on %s (n=%d); skipped.\", y, sub.shape[0])\n",
    "        return None\n",
    "\n",
    "    model = smf.ols(formula, data=sub)\n",
    "    res = model.fit(cov_type=\"HC3\")\n",
    "\n",
    "    coef = res.params.to_frame(\"coef\").join(res.bse.to_frame(\"se\")).join(res.pvalues.to_frame(\"pvalue\")).reset_index().rename(columns={\"index\":\"term\"})\n",
    "    coef_path = out_prefix.with_suffix(\"\").as_posix() + \"_coef.csv\"\n",
    "    coef.to_csv(coef_path, index=False)\n",
    "\n",
    "    summary_path = out_prefix.with_suffix(\"\").as_posix() + \"_summary.txt\"\n",
    "    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(res.summary().as_text())\n",
    "\n",
    "    return {\"formula\": formula, \"n\": int(res.nobs), \"rsq\": float(res.rsquared), \"adj_rsq\": float(res.rsquared_adj),\n",
    "            \"coef_csv\": coef_path, \"summary_txt\": summary_path}\n",
    "\n",
    "def rf_permutation_importance(df: pd.DataFrame, y: str, features: Sequence[str], out_csv: Path) -> Optional[pd.DataFrame]:\n",
    "    if not HAVE_SK:\n",
    "        logging.info(\"scikit-learn not installed; RF skipped.\")\n",
    "        return None\n",
    "    sub = df[list(features)+[y]].dropna()\n",
    "    if sub.shape[0] < 50:\n",
    "        logging.info(\"Too few rows for RF on %s (n=%d); skipped.\", y, sub.shape[0])\n",
    "        return None\n",
    "    X = sub[features].values\n",
    "    Y = sub[y].values.astype(float)\n",
    "    rf = RandomForestRegressor(n_estimators=400, max_depth=None, min_samples_leaf=3, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X, Y)\n",
    "    imp = permutation_importance(rf, X, Y, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "    tab = pd.DataFrame({\"feature\": features, \"perm_importance\": imp.importances_mean}).sort_values(\"perm_importance\", ascending=False)\n",
    "    tab.to_csv(out_csv, index=False)\n",
    "    return tab\n",
    "\n",
    "def parse_roi(s: Optional[str]) -> Dict[str, Tuple[Optional[float], Optional[float]]]:\n",
    "    if not s:\n",
    "        return {}\n",
    "    out = {}\n",
    "    parts = [p.strip() for p in s.split(\";\") if p.strip()]\n",
    "    for p in parts:\n",
    "        m = re.match(r\"^([A-Za-z0-9_]+)\\s*:\\s*\\[\\s*([-\\d\\.eE]+)?\\s*,\\s*([-\\d\\.eE]+)?\\s*\\]\\s*$\", p)\n",
    "        if not m:\n",
    "            continue\n",
    "        k = m.group(1)\n",
    "        lo = float(m.group(2)) if m.group(2) else None\n",
    "        hi = float(m.group(3)) if m.group(3) else None\n",
    "        out[k] = (lo, hi)\n",
    "    return out\n",
    "\n",
    "def apply_roi(df: pd.DataFrame, roi: Dict[str, Tuple[Optional[float], Optional[float]]]) -> pd.DataFrame:\n",
    "    sub = df.copy()\n",
    "    for k, (lo, hi) in roi.items():\n",
    "        if k not in sub.columns:\n",
    "            continue\n",
    "        if lo is not None:\n",
    "            sub = sub[sub[k] >= lo]\n",
    "        if hi is not None:\n",
    "            sub = sub[sub[k] <= hi]\n",
    "    return sub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcddfbf",
   "metadata": {},
   "source": [
    "# 读取与整形（Load & Tidy）\n",
    "读取转置 Excel 并预览\n",
    "\n",
    "- A–D 列为元信息（`row_type/eng/zh/flag`），E.. 为各试验列（数值）。  \n",
    "- 展示原始形状与前几行，确认数据结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8a2d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取与预览\n",
    "setup_logging(OUTDIR)\n",
    "logging.info(\"Loading Excel: %s [sheet=%s]\", INPUT_XLSX, SHEET_NAME)\n",
    "df_raw = load_transposed_excel(INPUT_XLSX, SHEET_NAME)\n",
    "print(\"df_raw.shape =\", df_raw.shape)\n",
    "# display(df_raw.head(8)) # 维度太大，暂不打印\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8f4aeb",
   "metadata": {},
   "source": [
    "# 转换为整洁表 tidy DataFrame（去重规则与元信息）\n",
    "\n",
    "- 依据 `eng` 去重；优先保留 `row_type ∈ {DerivedQuantity, Derived, opt}`。  \n",
    "- 输出：整洁表（每行一组试验）、行元信息 `row_meta`、摘要 `info`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f278a22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换为 tidy\n",
    "tidy_res = transposed_to_tidy(df_raw, keep_policy=KEEP_POLICY)\n",
    "tidy = tidy_res.tidy.copy()          # 含 'sample_id' 列\n",
    "row_meta = tidy_res.row_meta.copy()  # 选中的行字典\n",
    "info = tidy_res.info\n",
    "\n",
    "print(\"tidy.shape =\", tidy.shape, \"| n_runs =\", info[\"n_runs\"], \"| n_vars =\", info[\"n_vars\"])\n",
    "display(tidy.head(5))\n",
    "display(row_meta.head(10))\n",
    "\n",
    "# 保存中间结果\n",
    "save_csv(tidy, OUTDIR / \"tidy_raw.csv\")\n",
    "save_csv(row_meta, OUTDIR / \"row_meta.csv\")\n",
    "print(\"[Saved] tidy_raw.csv, row_meta.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0f3f57",
   "metadata": {},
   "source": [
    "# 别名解析与重命名（Alias Resolution & Renaming）\n",
    "别名映射并重命名到“规范名”（不丢失原列）\n",
    "\n",
    "- 从 `tidy` 提取全部变量列（除 `sample_id`），与内置 `CANONICAL_SYNONYMS` 做匹配。  \n",
    "- 保存 `aliases_used.json` 以便溯源；展示映射表的前若干行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea48a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解析别名与重命名\n",
    "cols_all = [c for c in tidy.columns if c != \"sample_id\"]\n",
    "\n",
    "# 可选：加载自定义别名 JSON（若有）\n",
    "CUSTOM_ALIASES_PATH = None  # Path(\"custom_aliases.json\")\n",
    "custom_aliases = None\n",
    "if CUSTOM_ALIASES_PATH and Path(CUSTOM_ALIASES_PATH).is_file():\n",
    "    with open(CUSTOM_ALIASES_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        custom_aliases = json.load(f)\n",
    "\n",
    "alias_map = resolve_aliases(cols_all, custom_aliases)\n",
    "save_json(alias_map, OUTDIR / \"aliases_used.json\")\n",
    "\n",
    "# 将“真实列名”→“规范名”，形成重命名映射\n",
    "canon_to_actual = {k: v for k, v in alias_map.items() if v is not None}\n",
    "actual_to_canon = {v: k for k, v in canon_to_actual.items()}\n",
    "renamed = tidy.rename(columns=actual_to_canon)\n",
    "df = renamed.copy()\n",
    "\n",
    "# 展示映射表\n",
    "alias_df = pd.DataFrame([\n",
    "    {\"canonical\": k, \"actual\": v if v is not None else \"(not found)\"}\n",
    "    for k, v in alias_map.items()\n",
    "]).sort_values(\"canonical\")\n",
    "display(alias_df.head(20))\n",
    "\n",
    "# 列名概览\n",
    "display(pd.DataFrame({\"columns\": df.columns.tolist()}).head(40))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208e482d",
   "metadata": {},
   "source": [
    "# 派生变量与范围校验（Derived & Validation）\n",
    "计算派生变量并进行范围检查\n",
    "\n",
    "- `D_calc = Lx1 - Lx2`  \n",
    "- `DiffWear_calc = (RW Wear - Curve) - (IRW Wear - Curve)`  \n",
    "- 若存在原生 `DiffWear`，给出与 `DiffWear_calc` 的差值统计。  \n",
    "- 根据预设 `RANGES` 做宽松范围校验，保存 `range_checks.json`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19f58c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 派生变量与校验\n",
    "RANGES = {\n",
    "    \"Lx1\": (0.0, 0.64), \"Lx2\": (0.0, 0.64), \"Lx3\": (-0.6, 0.4),\n",
    "    \"Kpx\": (50000, 15000000), \"Chx\": (100, 300000), \"Cld\": (10000, 200000),\n",
    "    \"Kld\": (2000000, 20000000), \"Ksx\": (15000, 500000),\n",
    "}\n",
    "\n",
    "'''\n",
    "参量项             定义\t            是否优化\t 基准值\t     下限\t     上限\n",
    "$_sprCpz    一系悬挂阻尼-垂向\t        1\t    10000.00 \t5000.00 \t60000.00 \n",
    "$_Kpx\t    一系悬挂刚度-纵向\t        1\t    800000.00 \t50000.00 \t15000000.00 \n",
    "$_Kpz\t    一系悬挂刚度-垂向\t        1\t    600000.00 \t500000.00 \t2000000.00 \n",
    "$_Ksx \t    二系悬挂刚度-纵向 \t        1\t    120000.00 \t15000.00 \t500000.00 \n",
    "$_Ksz\t    二系悬挂刚度-垂向\t        1\t    150000.00 \t50000.00 \t600000.00 \n",
    "$_Csz\t    二系悬挂阻尼-垂向\t        1\t    20000.00 \t5000.00 \t60000.00 \n",
    "$_Kld\t    横向减振器刚度\t            1\t    8000000.00 \t2000000.00 \t20000000.00  # 此下限值可能被善意突破\n",
    "$_Cld\t    横向减振器阻尼\t            1\t    50000.00 \t10000.00 \t200000.00 \n",
    "$_Chx\t    抗蛇行减振器阻尼\t        1\t    600000.00 \t100.00 \t    300000.00 \n",
    "$_Lx1\t    构架侧横向定位间距\t        1\t    0.20 \t    0\t        0.64\n",
    "$_Lx2\t    轴桥侧横向定位间距\t        1\t    0.58 \t    0\t        0.64\n",
    "$_Lx3\t    纵向定位调整值，可为负数     1\t     0.00 \t    -0.6\t     0.4\n",
    "\n",
    "'''\n",
    "\n",
    "# D_calc\n",
    "if \"Lx1\" in df.columns and \"Lx2\" in df.columns:\n",
    "    df[\"D_calc\"] = df[\"Lx1\"] - df[\"Lx2\"]\n",
    "\n",
    "# DiffWear_calc\n",
    "rw_col = \"RW Wear - Curve\" if \"RW Wear - Curve\" in df.columns else None\n",
    "irw_col = \"IRW Wear - Curve\" if \"IRW Wear - Curve\" in df.columns else None\n",
    "if rw_col and irw_col:\n",
    "    df[\"DiffWear_calc\"] = df[rw_col] - df[irw_col]\n",
    "\n",
    "if \"DiffWear\" in df.columns and \"DiffWear_calc\" in df.columns:\n",
    "    delta = (df[\"DiffWear\"] - df[\"DiffWear_calc\"]).abs()\n",
    "    df[\"DiffWear_match\"] = delta\n",
    "    print(\"DiffWear vs DiffWear_calc | median abs diff:\", float(delta.median()))\n",
    "\n",
    "# 范围检查\n",
    "validations = []\n",
    "for k, (lo, hi) in RANGES.items():\n",
    "    if k in df.columns:\n",
    "        v = df[k].dropna()\n",
    "        if v.empty:\n",
    "            continue\n",
    "        vmin, vmax = float(v.min()), float(v.max())\n",
    "        ok = True\n",
    "        if lo is not None and vmin < lo: ok = False\n",
    "        if hi is not None and vmax > hi: ok = False\n",
    "        validations.append({\"var\": k, \"min\": vmin, \"max\": vmax, \"range_ok\": ok})\n",
    "\n",
    "valid_df = pd.DataFrame(validations)\n",
    "display(valid_df)\n",
    "save_json({\"range_checks\": validations}, OUTDIR / \"range_checks.json\")\n",
    "\n",
    "# 保存整洁表（含派生列）\n",
    "save_csv(df, OUTDIR / \"tidy.csv\")\n",
    "print(\"[Saved] tidy.csv, range_checks.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768ba6c0",
   "metadata": {},
   "source": [
    "# 相关分析（Correlation）\n",
    "皮尔逊相关（设计变量 & 输出变量）\n",
    "\n",
    "- 设计变量 *design knobs*：`Lx1, Lx2, Lx3, Kpx, Chx, Cld, Kld, Ksx`（若存在）  \n",
    "- 输出/性能指标：磨耗与速度/舒适度（若存在）  \n",
    "- 同时输出 `Lx3` 与磨耗目标的成对相关。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ef361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 相关矩阵\n",
    "design_knobs = [c for c in [\"Lx1\",\"Lx2\",\"Lx3\",\"Kpx\",\"Chx\",\"Cld\",\"Kld\",\"Ksx\"] if c in df.columns]\n",
    "outcomes = [c for c in [\"RW Wear - Curve\",\"IRW Wear - Curve\",\"DiffWear\",\"DiffWear_calc\",\n",
    "                        \"RW Critical Vel - STR\",\"RW SperlingY - STR\",\"RW SperlingZ - STR\"] if c in df.columns]\n",
    "\n",
    "if design_knobs:\n",
    "    corr_knobs = pearson_corr(df, design_knobs)\n",
    "    display(Markdown(\"**设计变量相关矩阵（Pearson）**\"))\n",
    "    display(corr_knobs.round(3))\n",
    "    save_csv(corr_knobs, OUTDIR / \"corr_design_knobs.csv\")\n",
    "if outcomes:\n",
    "    corr_outcomes = pearson_corr(df, outcomes)\n",
    "    display(Markdown(\"**输出变量相关矩阵（Pearson）**\"))\n",
    "    display(corr_outcomes.round(3))\n",
    "    save_csv(corr_outcomes, OUTDIR / \"corr_outcomes.csv\")\n",
    "\n",
    "pairs = []\n",
    "if \"Lx3\" in df.columns:\n",
    "    for y in [c for c in [\"RW Wear - Curve\",\"IRW Wear - Curve\",\"DiffWear\",\"DiffWear_calc\"] if c in df.columns]:\n",
    "        r = df[[\"Lx3\", y]].dropna().corr().iloc[0,1]\n",
    "        pairs.append({\"X\":\"Lx3\",\"Y\":y,\"pearson_r\": float(r)})\n",
    "pairs_df = pd.DataFrame(pairs)\n",
    "display(Markdown(\"**Lx3 与磨耗目标的成对相关**\"))\n",
    "display(pairs_df)\n",
    "save_csv(pairs_df, OUTDIR / \"corr_Lx3_vs_targets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8376003",
   "metadata": {},
   "source": [
    "# 部分相关（Partial Correlation）\n",
    " 部分相关（控制变量 *controls*）\n",
    "\n",
    "- 方法：对 `x` 与 `y` 分别以 `controls` 作 OLS 残差化，然后计算皮尔逊相关。  \n",
    "- `SciPy` 可用时，给出 t 统计与双尾 p 值（自由度 `n - k - 2`）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3758928d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算部分相关\n",
    "pc_rows = []\n",
    "controls_used = [c for c in CONTROLS if c in df.columns]\n",
    "for (x, y) in [(\"Lx3\",\"RW Wear - Curve\"),(\"Lx3\",\"IRW Wear - Curve\"),\n",
    "               (\"Lx3\",\"DiffWear\"),(\"Lx3\",\"DiffWear_calc\")]:\n",
    "    if x in df.columns and y in df.columns:\n",
    "        r, p, n = partial_corr(df, x, y, controls=controls_used)\n",
    "        pc_rows.append({\"X\":x, \"Y\":y, \"controls\":\"+\".join(controls_used), \"pcorr\": r, \"pval\": p, \"n\": n})\n",
    "pc_df = pd.DataFrame(pc_rows)\n",
    "display(pc_df)\n",
    "save_csv(pc_df, OUTDIR / \"partial_corr.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e5f268",
   "metadata": {},
   "source": [
    "# 散点与拟合线（Scatter & Fit）\n",
    "绘制散点与线性拟合（并保存）\n",
    "\n",
    "- X 固定为 `Lx3`，Y 依次为：`RW Wear - Curve / IRW Wear - Curve / DiffWear / DiffWear_calc`（若存在）  \n",
    "- 每张图**先显示**，再保存至 `fig/` 目录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db307e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 逐一绘制并保存散点-拟合\n",
    "figdir = OUTDIR / \"fig\"\n",
    "figdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "targets = [c for c in [\"RW Wear - Curve\",\"IRW Wear - Curve\",\"DiffWear\",\"DiffWear_calc\"] if c in df.columns]\n",
    "for y in targets:\n",
    "    if \"Lx3\" in df.columns:\n",
    "        outpath = figdir / f\"Lx3_vs_{y.replace(' ', '_').replace('/', '-')}.png\"\n",
    "        scatter_with_fit(df, \"Lx3\", y, outpath, title=f\"Lx3 vs {y}\", show=True)\n",
    "        print(\"[Saved]\", outpath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede4b108",
   "metadata": {},
   "source": [
    "# OLS 回归（多项式与交互，HC3）\n",
    "OLS（含 poly(Lx3,2) 与交互），保存摘要与系数并展示关键结果\n",
    "\n",
    "- 目标 `y`：`RW Wear - Curve`、`IRW Wear - Curve`、`DiffWear_calc`  \n",
    "- 控制变量：`[\"Lx1\",\"Lx2\",\"Kpx\",\"Chx\"]`（存在即用）  \n",
    "- 交互项：`Lx3:Z`，其中 `Z` 来自以上控制变量  \n",
    "- 输出：`ols_<y>_coef.csv` 与 `ols_<y>_summary.txt`；在下方**打印摘要片段并展示系数表头"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077d2c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行 OLS 与展示结果\n",
    "ols_info = {}\n",
    "if \"Lx3\" in df.columns and HAVE_SM:\n",
    "    base_controls = [c for c in [\"Lx1\",\"Lx2\",\"Kpx\",\"Chx\"] if c in df.columns]\n",
    "    inters = [(\"Lx3\", z) for z in base_controls if z != \"Lx3\"]\n",
    "    for y in [c for c in [\"RW Wear - Curve\",\"IRW Wear - Curve\",\"DiffWear_calc\"] if c in df.columns]:\n",
    "        info = ols_lx3_model(df, y=y, controls=base_controls, interactions=inters,\n",
    "                             out_prefix=OUTDIR / f\"ols_{y.replace(' ', '_')}\")\n",
    "        if info:\n",
    "            ols_info[y] = info\n",
    "            # 展示系数表头\n",
    "            coef_df = pd.read_csv(info[\"coef_csv\"])\n",
    "            display(Markdown(f\"**OLS 系数表（{y}）**\"))\n",
    "            display(coef_df.head(10))\n",
    "            # 打印摘要的前若干行\n",
    "            with open(info[\"summary_txt\"], \"r\", encoding=\"utf-8\") as f:\n",
    "                summary_text = \"\".join(f.readlines()[:40])\n",
    "            print(\"—— 摘要片段 ——\")\n",
    "            print(summary_text)\n",
    "\n",
    "# 保存元信息\n",
    "save_json({\"ols\": ols_info}, OUTDIR / \"ols_meta.json\")\n",
    "print(\"[Saved] ols_meta.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23368bb2",
   "metadata": {},
   "source": [
    "# 共线性与随机森林（VIF & Random Forest）\n",
    "VIF（方差膨胀因子）与随机森林置换重要性（可选）\n",
    "- VIF：需 `statsmodels`；当样本量足够且变量数 ≥ 2 时计算。  \n",
    "- RF 重要性：需 `scikit-learn`；样本量不足则跳过。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdec13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIF 与 RF\n",
    "vif_features = [c for c in [\"Lx1\",\"Lx2\",\"Lx3\",\"Kpx\",\"Chx\",\"Cld\",\"Kld\",\"Ksx\"] if c in df.columns]\n",
    "vifdf = vif_table(df, vif_features)\n",
    "if vifdf is not None:\n",
    "    save_csv(vifdf, OUTDIR / \"vif_design_knobs.csv\")\n",
    "    display(Markdown(\"**VIF（设计变量）**\"))\n",
    "    display(vifdf)\n",
    "\n",
    "rf_features = [c for c in [\"Lx1\",\"Lx2\",\"Lx3\",\"Kpx\",\"Chx\",\"Cld\",\"Kld\",\"Ksx\"] if c in df.columns]\n",
    "if rf_features:\n",
    "    for y in [c for c in [\"RW Wear - Curve\",\"IRW Wear - Curve\",\"DiffWear_calc\"] if c in df.columns]:\n",
    "        tab = rf_permutation_importance(df, y, rf_features, OUTDIR / f\"rf_perm_importance_{y.replace(' ', '_')}.csv\")\n",
    "        if tab is not None:\n",
    "            display(Markdown(f\"**RF 置换重要性（{y}）**\"))\n",
    "            display(tab.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74e6c28",
   "metadata": {},
   "source": [
    "## RF 置换重要性（Permutation Importance）解读\n",
    "功能：衡量模型对某一特征的依赖度（预测贡献）。做法是把该特征打乱（破坏其信息），比较模型分数（回归常用 R^2）的下降量：\n",
    "\n",
    "Ij = Score(X,y)−Score(πj(X),y), πj表示只打乱第 j 列。可读信息：数值越大，模型越“离不开”该特征。与 线性回归的显著性不同，它是预测层面的度量，不是“系数为 0 否”的假设检验。\n",
    "\n",
    "## 结果摘要：\n",
    "- RW：Kpx(1.52) ≫ Lx1(0.60) ≳ Lx3(0.50) ≫ 其余 → Kpx 是最强驱动因子；\n",
    "- IRW：Lx2(1.19) ≈ Kpx(1.18) > Lx1(0.83) > Lx3(0.67) → Lx2 与 Kpx 双强；\n",
    "- DiffWear：Kpx(1.52) > Lx1(0.86) > Lx3(0.45) → 仍是 Kpx 主导。\n",
    "\n",
    "提醒：这些数值是在各自目标 y 下的下降量，不同 y 之间不可直接比较；只在同一目标内比较相对大小。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d3684d",
   "metadata": {},
   "source": [
    "# ROI 斜率估计（Slope within ROI）\n",
    "在 ROI 内拟合 `DiffWear`（或 `DiffWear_calc`）对 `Lx3` 的线性斜率\n",
    "\n",
    "- 解析 ROI 字符串，筛选子集后用最小二乘估计斜率。  \n",
    "- 若 `statsmodels` 不可用，则回退至 `numpy.linalg.lstsq`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2666b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算 ROI 斜率并保存\n",
    "roi = parse_roi(ROI_STRING)\n",
    "roi_meta = {}\n",
    "if roi and \"Lx3\" in df.columns and (\"DiffWear_calc\" in df.columns or \"DiffWear\" in df.columns):\n",
    "    yvar = \"DiffWear_calc\" if \"DiffWear_calc\" in df.columns else \"DiffWear\"\n",
    "    sub = apply_roi(df, roi)[[\"Lx3\", yvar]].dropna()\n",
    "    if sub.shape[0] >= 10:\n",
    "        if HAVE_SM:\n",
    "            X = sm.add_constant(sub[\"Lx3\"].values)\n",
    "        else:\n",
    "            X = np.column_stack([np.ones(len(sub)), sub[\"Lx3\"].values])\n",
    "        beta, *_ = np.linalg.lstsq(X, sub[yvar].values.astype(float), rcond=None)\n",
    "        slope = float(beta[-1])\n",
    "        roi_meta = {\"roi\": roi, \"n\": int(sub.shape[0]), \"y\": yvar, \"slope_dDeltaWear_dLx3\": slope}\n",
    "        save_json(roi_meta, OUTDIR / \"roi_slope_DiffWear_vs_Lx3.json\")\n",
    "        print(\"[ROI] d(ΔWear)/d(Lx3) =\", slope, \"| n =\", sub.shape[0])\n",
    "    else:\n",
    "        print(\"[ROI] 样本不足，跳过（n<10）\")\n",
    "else:\n",
    "    print(\"[ROI] 未设定或缺少必要列，跳过。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedf41d3",
   "metadata": {},
   "source": [
    "# 输出清单（Artifacts）\n",
    "输出文件总览（按文件大小降序）\n",
    "\n",
    "- 便于快速定位已生成的 CSV/JSON/PNG 等文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141fa1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 列出 OUTDIR 内容\n",
    "files = []\n",
    "for p in OUTDIR.rglob(\"*\"):\n",
    "    if p.is_file():\n",
    "        files.append({\"path\": str(p.relative_to(OUTDIR)), \"size_kb\": round(p.stat().st_size/1024, 1)})\n",
    "out_df = pd.DataFrame(files).sort_values(\"size_kb\", ascending=False)\n",
    "display(out_df.head(50))\n",
    "print(f\"[Done] 输出目录：{OUTDIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pypack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
